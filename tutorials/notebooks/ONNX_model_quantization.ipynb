{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <FONT COLOR=\"#273B5F\"> Quantization of Deep Learning Models using ONNX Runtime: </h1>\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The process of quantization involves the convertion the original floating-point parameters and intermediate activations of a model into lower precision integer representations. This reduction in precision can significantly decrease the memory footprint and computational cost of the model, making it more efficient to deploy on STM32 board using STM32Cube.AI or any other resource-constrained devices.\n",
    "\n",
    "ONNX Runtime Quantization is a feature the ONNX Runtime that allows efficient execution of quantized models. It provides tools and techniques to quantize the ONNX format models. It includes methods for quantizing weights and activations.\n",
    "\n",
    "\n",
    "**This notebook demonstrates the process of static post-training quantization for deep learning models using the ONNX runtime. It covers the model quantization with calibration dataset or with fake data, the evaluation of the full precision model and the quantized model, and then the STM32Cube.AI Developer Cloud is used to benchmark the models and to generate the model C code to be deployed on your STM32 board.** \n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "This software component is licensed by ST under BSD-3-Clause license,\n",
    "the \"License\"; \n",
    "\n",
    "You may not use this file except in compliance with the\n",
    "License. \n",
    "\n",
    "You may obtain a copy of the License at: https://opensource.org/licenses/BSD-3-Clause\n",
    "\n",
    "Copyright (c) 2023 STMicroelectronics. All rights reserved"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid #273B5F\">\n",
    "<h2>Table of content</h2>\n",
    "<ul style=\"list-style-type: none\">\n",
    "  <li><a href=\"#settings\">1. Settings</a>\n",
    "  <ul style=\"list-style-type: none\">\n",
    "    <li><a href=\"#install\">1.1 Install and import necessary packages</a></li>\n",
    "    <li><a href=\"#select\">1.2 Select input model filename and dataset folder</a></li>\n",
    "  </ul>\n",
    "</li>\n",
    "<li><a href=\"#quantization\">2.Quantization</a></li>\n",
    "      <ul style=\"list-style-type: none\">\n",
    "    <li><a href=\"#opset\">2.1 Opset conversion</a></li>\n",
    "    <li><a href=\"#dataset\">2.2 Creating calibration dataset</a></li>\n",
    "    <li><a href=\"#quantize\">2.3 Quantize the model using QDQ quantization to int8 weights and activations</a></li>\n",
    "  </ul>\n",
    "<li><a href=\"#validation\">3. Evaluation </a></li>\n",
    "<li><a href=\"#benchmark\">4. Benchmark</a></li>\n",
    "      <ul style=\"list-style-type: none\">\n",
    "    <li><a href=\"#proxy\">4.1 Proxy setting and connection to the STM32Cube.AI Developer Cloud</a></li>\n",
    "    <li><a href=\"#analyze\">4.2 Analyze your model memory footprints</a></li>\n",
    "    <li><a href=\"#Benchmark\">4.3 Benchmark your model on a STM32 target</a></li>\n",
    "    <li><a href=\"#generate\">4.4 Generate the model optimized C code for STM32</a></li>\n",
    "         \n",
    "\n",
    "  </ul>\n",
    "</ul>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"settings\">\n",
    "    <h2>1. Settings</h2>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"install\">\n",
    "    <h3>1.1 Install and import necessary packages </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy==1.23.5\n",
    "!{sys.executable} -m pip install onnxruntime==1.13.1\n",
    "!{sys.executable} -m pip install onnx==1.12.0\n",
    "!{sys.executable} -m pip install Pillow==9.4.0\n",
    "!{sys.executable} -m pip install tensorflow==2.8.3 \n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "!{sys.executable} -m pip install tqdm\n",
    "\n",
    "# for the cloud service\n",
    "!{sys.executable} -m pip install gitdir\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import onnx\n",
    "import onnxruntime\n",
    "from onnx import version_converter\n",
    "from onnxruntime import quantization\n",
    "from onnxruntime.quantization import (CalibrationDataReader, CalibrationMethod,\n",
    "                                      QuantFormat, QuantType, quantize_static)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"select\">\n",
    "    <h3>1.2 Select input model filename and dataset folder</h3>\n",
    "</div>\n",
    "\n",
    "\n",
    "The code section bellow is to set the paths of the model and the dataset for the following notebook, the model is expected to be in Open Neural Network Exchange (ONNX) format, in the conducted experience we are using the mobilenet_v2_0.35_128 model as an exemple with the modified version of COCO2014 dataset. To find more details please visit this [link](https://pjreddie.com/projects/coco-mirror/). \n",
    "\n",
    "The quantization set is a directory containing a sub-directory per class, For instance:\n",
    "\n",
    "```bash\n",
    " quantization_set/\n",
    " ..class_a:person/\n",
    " ....a_image_1.jpg\n",
    " ....a_image_2.jpg\n",
    " ..class_b:not_person/\n",
    " ....b_image_1.jpg\n",
    " ....b_image_2.jpg\n",
    "\n",
    " \n",
    "```\n",
    "**For proper quantization, ``quantization_dataset_path`` must point to the quatization set or to the training set to create the calibration dataset later.**\n",
    "\n",
    "**For fake quantization, ``quantization_dataset_path``  is set to ``None``.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_model =\"models\\mobilenet_v2_128_0.5.onnx\"\n",
    "quantization_dataset_path=os.path.join(\"..\\..\\image_classification\\scripts\\training\\datasets\\person_dataset\\quantization_set\")\n",
    "#quantization_dataset_path=None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"quantization\">\n",
    "    <h2>2. Quantization</h2>\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"opset\">\n",
    "    <h3>2.1. Opset conversion  </h3>\n",
    "</div>\n",
    "\n",
    "The next function is to change the opset number of the model, this can be a necessary step to ensure a proper quantization. \n",
    "\n",
    "Since Batch normalization folding and other advanced optimizations are available for models with opset 13 and above and to be aligned with the onnx and onnx runtime versions, we are converting the opset of the model to 15.\n",
    "\n",
    "To ensure compatibility between ONNX runtime version and the opset number check [the official documentation of ONNX Runtime](https://onnxruntime.ai/docs/reference/compatibility.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_opset(input_model, new_opset): \n",
    "    \n",
    "    if not input_model.endswith('.onnx'):\n",
    "        raise Exception(\"Error! The model must be in onnx format\")    \n",
    "    model = onnx.load(input_model)\n",
    "    # Check the current opset version\n",
    "    current_opset = model.opset_import[0].version\n",
    "    if current_opset == new_opset:\n",
    "        print(f\"The model is already using opset {new_opset}\")\n",
    "        return input_model\n",
    "\n",
    "    # Modify the opset version in the model\n",
    "    converted_model = version_converter.convert_version(model, new_opset)\n",
    "    temp_model_path = input_model+ '.temp'\n",
    "    onnx.save(converted_model, temp_model_path)\n",
    "\n",
    "    # Load the modified model using ONNX Runtime Check if the model is valid\n",
    "    session = onnxruntime.InferenceSession(temp_model_path)\n",
    "    try:\n",
    "        session.get_inputs()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the modified model: {e}\")\n",
    "        return\n",
    "\n",
    "    # Replace the original model file with the modified model\n",
    "    os.replace(temp_model_path, input_model)\n",
    "    print(f\"The model has been converted to opset {new_opset} and saved at the same location.\")\n",
    "    return input_model\n",
    "    \n",
    "change_opset(input_model, new_opset=15)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"dataset\">\n",
    "    <h3> 2.2 Creating calibration dataset </h3>\n",
    "</div>\n",
    "\n",
    "During the ONNX runtime quantization, the model is run on the calibration data to provide statistics about the dynamic and characteristics of each input and output. These statistics are then used to determine the main quantization parameters, which are the scale factor and a zero-point or offset to to map the floating-point values to integers. \n",
    "\n",
    "When obtaining representative real data for calibration is difficult or impractical, randomly generated or synthetic input data can be used for the calibration. \n",
    "\n",
    "The next three code sections bellow contain:\n",
    "* The `create_calibration_dataset` function to create the calibration set from the original directory by taking a specific number of samples from each class, and the preprocess_image_batch function to load the batch and process it. \n",
    "* The `preprocess_random_images` to generate random images for fake quantization and preprocess them.\n",
    "* The `ImageNetDataReader` class that inherate from the ONNX Runtime calibration data readers and implement the `get_next method` to generate and provide input data dictionaries for the calibration process.\n",
    "\n",
    "\n",
    "As precised in <a href=\"#select\"> Select input model filename and dataset folder</a> if you want to perfomre quantization with fake data, set the ``dataset_path`` to ``NONE``.\n",
    "\n",
    "\n",
    "**Note :** the preprocessing of the quantization dataset in the section bellow is aligned with preprocessing of the trained model, for other models with diffrent preprocessing schema some arguments need to be changed like the ``color_mode``, ``interpolation`` and ``norm`` for the normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_calibration_dataset(dataset_path, samples_per_class = 100):\n",
    "    # the calibration dataset will be find in under the same directory as the dataset \n",
    "    calibration_dataset_path = os.path.join(os.path.dirname(dataset_path), 'calibration_' + os.path.basename(dataset_path))\n",
    "    # List directories\n",
    "    dir_list = next(os.walk(dataset_path))[1]\n",
    "\n",
    "    # Create the target directory if it doesn't exist\n",
    "    if not os.path.exists(calibration_dataset_path):\n",
    "        os.makedirs(calibration_dataset_path)\n",
    "\n",
    "    # For each directory, create a new directory in the target directory\n",
    "    for dir_i in tqdm(dir_list):\n",
    "        img_list = glob.glob(os.path.join(dataset_path, dir_i, '*.jpg')) + \\\n",
    "                   glob.glob(os.path.join(dataset_path, dir_i, '*.png')) + \\\n",
    "                   glob.glob(os.path.join(dataset_path, dir_i, '*.jpeg'))\n",
    "\n",
    "        # Shuffle the data\n",
    "        random.shuffle(img_list)\n",
    "\n",
    "        # Copy a subset of images to the target directory\n",
    "        for j in range(min(samples_per_class, len(img_list))):\n",
    "            shutil.copy2(img_list[j], calibration_dataset_path)\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(current_time + ' - ' + f'Done creating calibration dataset.')\n",
    "    return(calibration_dataset_path)\n",
    "\n",
    "def preprocess_image_batch(images_folder: str, height: int, width: int,  interpolation = 'bilinear', norm='tf', size_limit=0):\n",
    "    \"\"\"\n",
    "    Loads a batch of images and preprocess them\n",
    "    parameter images_folder: path to folder storing images\n",
    "    parameter height: image height in pixels\n",
    "    parameter width: image width in pixels\n",
    "    parameter size_limit: number of images to load. Default is 0 which means all images are picked.\n",
    "    return: list of matrices characterizing multiple images\n",
    "    \"\"\"\n",
    "    TORCH_MEANS = [0.485, 0.456, 0.406]\n",
    "    TORCH_STD = [0.224, 0.224, 0.224]\n",
    "\n",
    "    image_names = os.listdir(images_folder)\n",
    "    if size_limit > 0 and len(image_names) >= size_limit:\n",
    "        batch_filenames = [image_names[i] for i in range(size_limit)]\n",
    "    else:\n",
    "        batch_filenames = image_names\n",
    "    unconcatenated_batch_data = []\n",
    "\n",
    "    for image_name in batch_filenames:\n",
    "        image_filepath = images_folder + \"/\" + image_name\n",
    "        img = tf.keras.utils.load_img(image_filepath , grayscale = False, color_mode = 'rgb',\n",
    "            target_size = (width,height), interpolation=interpolation)\n",
    "        img_array = np.array([tf.keras.utils.img_to_array(img)])\n",
    "        if norm.lower() == 'tf':\n",
    "            img_array = -1 + img_array / 127.5\n",
    "        elif norm.lower() == 'torch':\n",
    "            img_array = img_array / 255.0\n",
    "            img_array = img_array - TORCH_MEANS\n",
    "            img_array = img_array/ TORCH_STD\n",
    "        # transpose the data (hwc to chw) to be conform to the expected input data representation\n",
    "        img_array = img_array.transpose((0,3,1,2))\n",
    "        unconcatenated_batch_data.append(img_array)\n",
    "    batch_data = np.stack(unconcatenated_batch_data, axis=0)\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_random_images(height: int, width: int, channel: int,  size_limit=400):\n",
    "    \"\"\"\n",
    "    Loads a batch of images and preprocess them\n",
    "    parameter height: image height in pixels\n",
    "    parameter width: image width in pixels\n",
    "    parameter size_limit: number of images to load. Default is 100\n",
    "    return: list of matrices characterizing multiple images\n",
    "    \"\"\"\n",
    "    unconcatenated_batch_data = []\n",
    "    for i in range(size_limit):\n",
    "        random_vals = np.random.uniform(0, 1, channel*height*width).astype('float32')\n",
    "        random_image = random_vals.reshape(1, channel, height, width)\n",
    "        unconcatenated_batch_data.append(random_image)\n",
    "        batch_data = np.concatenate(np.expand_dims(unconcatenated_batch_data, axis=0), axis=0) \n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(current_time + ' - ' + 'random dataset with {} random images'.format(size_limit))\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetDataReader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_image_folder: str, model_path: str):\n",
    "        # Use inference session to get input shape\n",
    "        session = onnxruntime.InferenceSession(model_path, None)\n",
    "        (_, channel, height, width) = session.get_inputs()[0].shape\n",
    "\n",
    "        # Convert image to input data\n",
    "        if calibration_image_folder:\n",
    "            self.nhwc_data_list = preprocess_image_batch(\n",
    "                calibration_image_folder, height, width, norm='tf', size_limit=0\n",
    "            )\n",
    "        else:\n",
    "            self.nhwc_data_list = preprocess_random_images(\n",
    "                height, width, channel\n",
    "            )\n",
    "\n",
    "        self.input_name = session.get_inputs()[0].name\n",
    "        self.datasize = len(self.nhwc_data_list)\n",
    "\n",
    "        self.enum_data = None  # Enumerator for calibration data\n",
    "\n",
    "    def get_next(self):\n",
    "        if self.enum_data is None:\n",
    "            # Create an iterator that generates input dictionaries\n",
    "            # with input name and corresponding data\n",
    "            self.enum_data = iter(\n",
    "                [{self.input_name: nhwc_data} for nhwc_data in self.nhwc_data_list]\n",
    "            )\n",
    "        \n",
    "        return next(self.enum_data, None)  # Return next item from enumerator\n",
    "\n",
    "    def rewind(self):\n",
    "        self.enum_data = None  # Reset the enumeration of calibration dataclass ImageNetDataReader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"quantize\">\n",
    "    <h3> 2.3 Quantize the model using QDQ quantization to int8 weights and activations </h3>\n",
    "</div>\n",
    "\n",
    "The following section quantize the float32 onnx model to int8 quantized onnx model after the preprocessing to prepare it to the qunatization by using the ``quantize_static`` function that we recommand to use with calibration data and with the following supported arguments setting.\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th style=\"text-align: left\">Argument</th>\n",
    "<th style=\"text-align: left\">Description /  CUBE.AI recommendation</th>\n",
    "</tr>\n",
    "    \n",
    "<tr><td style=\"text-align: left\">Quant_format </td>\n",
    "<td style=\"text-align: left\"> <p> QuantFormat.QDQ format: <strong>recommended</strong>, it quantizes the model by inserting QuantizeLinear/DeQuantizeLinear on the tensor. QOperator format: <strong> not recommended </strong>, it quantizes the model with quantized operators directly </p> </td></tr>\n",
    "<tr><td style=\"text-align: left\"> Activation type</td> \n",
    "<td style=\"text-align: left\"> <p> QuantType.QInt8: <strong>recommended</strong>, it quantizes the activations to int8.  QuantType.QUInt8: <strong>not recommended</strong>, to quantize the activations uint8 </p> </td></tr>  \n",
    "<tr><td style=\"text-align: left\">Weight_type </td> \n",
    "<td style=\"text-align: left\"> <p> QuantType.QInt8: <strong>recommended</strong> , it quantizes the weights to int8.  QuantType.QUInt8: <strong>not recommended</strong>, it quantizes the weights to uint8</p> </td></tr> \n",
    "<tr><td style=\"text-align: left\">Per_Channel</td>\n",
    "<td style=\"text-align: left\"> <p>True: <strong>recommended</strong>, it makes the quantization process is carried out individually and separately for each channel based on the characteristics of the data within that specific channel / False: supported and <strong>not recommended</strong>, the quantization process is carried out for each tensor </p> </td>\n",
    "</tr>\n",
    "<tr><td style=\"text-align: left\">ActivationSymmetric</td>\n",
    "<td style=\"text-align: left\"> <p>`False: <strong>recommended</strong> it makes the activations in the range of [-128  +127]. True: supported, it makes the  activations in the range of [-128  +127] with the zero_point=0 </p> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: left\">WeightSymmetric</td>\n",
    "<td style=\"text-align: left\"> <p>True: <strong>Highly recommended</strong>, it makes the weights in the range of [-128  +127] with the zero_point=0.  False: supported and <strong>not recommended</strong>, it makes the weights in the range of [-128  +127]</p> </td>\n",
    "</tr>\n",
    "   \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not quantization_dataset_path is None:\n",
    "    calibration_dataset_path=create_calibration_dataset(quantization_dataset_path, samples_per_class = 100)\n",
    "else: calibration_dataset_path= None\n",
    "# set the data reader pointing to the representative dataset\n",
    "print('Prepare the data reader for the representative dataset...')\n",
    "dr = ImageNetDataReader(calibration_dataset_path, input_model) \n",
    "print('the data reader is ready')\n",
    "\n",
    "# preprocess the model to infer shapes of each tensor\n",
    "infer_model = os.path.splitext(input_model)\n",
    "infer_model = infer_model[0] + '_infer' + infer_model[1]\n",
    "print('Infer for the model: {}...'.format(os.path.basename(input_model)))\n",
    "quantization.quant_pre_process(input_model_path=input_model, output_model_path=infer_model, skip_optimization=False)\n",
    "\n",
    "# prepare quantized onnx model filename\n",
    "quant_model = os.path.splitext(input_model)\n",
    "if not calibration_dataset_path is None:\n",
    "    quant_model = quant_model[0] + '_QDQ_quant' + quant_model[1]\n",
    "else:\n",
    "    quant_model = quant_model[0] + '_QDQ_fakequant' + quant_model[1]\n",
    "print('Quantize the model {}, please wait...'.format(os.path.basename(input_model)))\n",
    "\n",
    "quantize_static(\n",
    "        infer_model,\n",
    "        quant_model,\n",
    "        dr,\n",
    "        calibrate_method=CalibrationMethod.MinMax, \n",
    "        quant_format=QuantFormat.QDQ,\n",
    "        per_channel=True,\n",
    "        weight_type=QuantType.QInt8, \n",
    "        activation_type = QuantType.QInt8, \n",
    "        optimize_model=False,\n",
    "        reduce_range=True,\n",
    "        extra_options={'WeightSymmetric': True, 'ActivationSymmetric':False, })\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(current_time + ' - ' + '{} model has been created'.format(os.path.basename(quant_model)))\n",
    "quantized_session = onnxruntime.InferenceSession(quant_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"validation\">\n",
    "        <h2> 3. Evaluation </h2>\n",
    "</div>\n",
    "\n",
    "\n",
    "The bellow code section contains some functions to evaluate the models on the validation dataset.\n",
    "\n",
    "**Note:** again, the preprocessing of the evaluation dataset in the section bellow is aligned with preprocessing of the trained model, for other models with diffrent preprocessing schema some arguments need to be changed like the ``color_mode``, ``interpolation`` and ``norm`` for the normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx import ModelProto\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TORCH_MEANS = [0.485,0.456,0.406]\n",
    "TORCH_STD = [0.224, 0.224, 0.224]\n",
    "\n",
    "\n",
    "def get_preprocessed_image(image_path , height, width, grayscale, color_mode, interpolation, norm):\n",
    "    img = tf.keras.utils.load_img(image_path, grayscale=grayscale , color_mode = color_mode,\n",
    "     target_size = (width,height), interpolation=interpolation)\n",
    "    img_array = np.array([tf.keras.utils.img_to_array(img)])\n",
    "    if norm.lower() == 'tf':\n",
    "        img_array = -1 + img_array / 127.5\n",
    "    elif norm.lower() == 'torch':\n",
    "        img_array = img_array / 255.0\n",
    "        img_array = img_array - TORCH_MEANS\n",
    "        img_array= img_array/ TORCH_STD\n",
    "    img_array = img_array.transpose((0,3,1,2))\n",
    "    return img_array\n",
    "\n",
    "def predict_onnx(sess, data):\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    label_name = sess.get_outputs()[0].name\n",
    "    onx_pred = sess.run([label_name], {input_name: data.astype(np.float32)})[0]\n",
    "    return onx_pred\n",
    "\n",
    "def plot_confusion_matrix(cm, class_labels, model_name, val_accuracy = '_'):\n",
    "    print(f'confusion_matrix : \\n{cm}')\n",
    "    cm_normalized = [element/sum(row) for element, row in zip([row for row in cm], cm)]\n",
    "    cm_normalized = np.array(cm_normalized)\n",
    "    \n",
    "    plt.figure(figsize = (4,4))\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(cm_normalized)\n",
    "    disp.plot(cmap = \"Blues\")\n",
    "    plt.title(f'Model_accuracy : {val_accuracy}', fontsize = 10)\n",
    "    plt.tight_layout(pad=3)\n",
    "    plt.ylabel(\"True labels\")\n",
    "    plt.xlabel(\"Predicted labels\")\n",
    "    plt.xticks(np.arange(0, len(class_labels)), class_labels)\n",
    "    plt.yticks(np.arange(0, len(class_labels)), class_labels)\n",
    "    plt.savefig(f'{model_name}_confusion-matrix.png')\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_onnx_model(onnx_model_path, val_dir, model_name, interpolation = 'bilinear'):\n",
    "    onx = ModelProto()\n",
    "    with open(onnx_model_path, mode = 'rb') as f:\n",
    "        content = f.read()\n",
    "        onx.ParseFromString(content)\n",
    "    sess = onnxruntime.InferenceSession(onnx_model_path)\n",
    "    (_, _, img_height, img_width) = sess.get_inputs()[0].shape\n",
    "    gt_labels = []\n",
    "    prd_labels = np.empty((0))\n",
    "    class_labels = sorted(os.listdir(val_dir))\n",
    "    for i in range(len(class_labels)):\n",
    "        class_label = class_labels[i]\n",
    "        \n",
    "        for file in os.listdir(os.path.join(val_dir, class_label)):\n",
    "            gt_labels.append(i)\n",
    "            image_path = os.path.join(val_dir,class_label,file)\n",
    "            # don't forget to adapt the preprocessing schema\n",
    "            img = get_preprocessed_image(image_path, width = img_width, height = img_height, \n",
    "                                          grayscale = False, color_mode = 'rgb',\n",
    "                                          interpolation = interpolation, norm='tf')\n",
    "            # predicting the results on the batch\n",
    "            pred = predict_onnx(sess, img).argmax(axis = 1)\n",
    "            prd_labels = np.concatenate((prd_labels, pred))\n",
    "\n",
    "    val_acc = round(accuracy_score(gt_labels, prd_labels), 6)\n",
    "    print(f'Evaluation Top 1 accuracy : {val_acc}')\n",
    "    val_cm = confusion_matrix(gt_labels,prd_labels)\n",
    "\n",
    "    plot_confusion_matrix(val_cm, class_labels, model_name, val_accuracy = val_acc)\n",
    "    \n",
    "    return val_acc, val_cm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The input_model should be set to the model to be evaluated and the val_set to validation dataset.**\n",
    "\n",
    "\n",
    "Evaluation of the full precision model: the floating-point numbers are used to represent weights, activations, and computations. This evaluation provides a baseline measure of the model's accuracy in its original form without any quantization applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set= os.path.join(\"..\\..\\image_classification\\scripts\\training\\datasets\\person_dataset\\val_set\")\n",
    "input_model =\"models\\\\mobilenet_v2_128_0.5.onnx\"\n",
    "evaluate_onnx_model(input_model, val_set, model_name='float_model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the quantized model with the original dataset: it is evaluated using the same validation dataset to measure its accuracy to see the potential impact of quantization on the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_model =\"models\\\\mobilenet_v2_128_0.5_model_QDQ_quant.onnx\"\n",
    "evaluate_onnx_model(input_model, val_set, model_name='int8_model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the quantized model with the fake data: the ``mobilenet_v2_128_0.5_QDQ_fakequant.onnx`` has to be created by reproducing the same experience with ``data_path`` set to ``NONE``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_model=\"models\\\\mobilenet_v2_128_0.5_model_QDQ_fakequant.onnx\"\n",
    "evaluate_onnx_model(input_model, val_set, model_name='int8_model_fake_data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th style=\"text-align: left\">Metric </th>\n",
    "<th style=\"text-align: left\">Value</th>\n",
    "\n",
    "</tr>\n",
    "<tr>\n",
    "    \n",
    "    \n",
    "<td style=\"text-align: left\">float_model     </td>\n",
    "<td style=\"text-align: left\"> 0.907203</td>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td style=\"text-align: left\">quant_model_real_data </td>\n",
    "<td style=\"text-align: left\">0.9053531</td>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td style=\"text-align: left\">quant_model_fake_data</td>\n",
    "<td style=\"text-align: left\">0.725064</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "    \n",
    "</table>\n",
    "\n",
    "\n",
    "Based on the given results, it can be concluded that quantization with real data has a negligible impact on the accuracy of the model compared to the float model. However, quantization with fake data leads to a notable decrease in accuracy, because the fake data we use for calibration does not accurately represent the actual distribution or characteristics of the data the model will encounter during inference.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"benchmark\">\n",
    "        <h2> 4. Benchmarking the model on the STM32Cube.AI Developer Cloud</h2>\n",
    "</div>\n",
    "\n",
    "In this section we use the [STM32Cube.AI Developer Cloud](https://stm32ai-cs.st.com/home) to analyze, optimize, benchmark and deploy quantized neural network on a **STM32** target.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div id=\"proxy\">\n",
    "        <h3> 4.1 Proxy setting and connection to the STM32Cube.AI Developer Cloud</h3>\n",
    "</div>\n",
    "\n",
    "If you are behind a proxy, you can uncomment and fill the following proxy settings.\n",
    "\n",
    "**NOTE**: If the password contains some special characters like `@`, `:` etc. they need to be url-encoded with their ASCII values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['http_proxy'] = \"http://user:passwd@ip_address:port\"\n",
    "# os.environ['https_proxy'] = \"https://user:passwd@ip_address:port\"\n",
    "# And eventually disable SSL verification\n",
    "# os.environ['NO_SSL_VERIFY'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import getpass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get STM32Cube.AI Developer Cloud\n",
    "!gitdir https://github.com/STMicroelectronics/stm32ai-modelzoo/tree/main/common/stm32ai_dc\n",
    "\n",
    "# Reorganize local folders\n",
    "if os.path.exists('./stm32ai_dc'):\n",
    "    shutil.rmtree('./stm32ai_dc')\n",
    "shutil.move('./common/stm32ai_dc', './stm32ai_dc')\n",
    "shutil.rmtree('./common')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('stm32ai'))\n",
    "os.environ['STATS_TYPE'] = 'jupyter_devcloud'\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "from stm32ai_dc import (CliLibraryIde, CliLibrarySerie, CliParameters,\n",
    "                        CloudBackend, Stm32Ai)\n",
    "from stm32ai_dc.errors import BenchmarkServerError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an account on **myST** and then sign in to [STM32Cube.AI Developer Cloud](https://stm32ai-cs.st.com/home) to be able access the service and then set the environment variables below with your credentials: the mail adress should be set as a string in username and a popup will appear to enter the password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username ='xxx.yyy@st.com'\n",
    "os.environ['stmai_username'] = username\n",
    "print('Enter you password')\n",
    "password = getpass.getpass()\n",
    "os.environ['stmai_password'] = password\n",
    "os.environ['NO_SSL_VERIFY'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log in STM32Cube.AI Developer Cloud \n",
    "try:\n",
    "    stmai = Stm32Ai(CloudBackend(str(username), str(password)))\n",
    "    print(\"Successfully Connected!\")\n",
    "except Exception as e:\n",
    "    print(\"ERROR: \", e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the model path you want to conduct the benchmark on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload the model on STM32Cube.AI Developer Cloud\n",
    "model_path = quant_model\n",
    "stmai.upload_model(model_path)\n",
    "model_name = os.path.basename(model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"analyze\">\n",
    "        <h3> 4.2 Analyze the model memory footprints</h3>\n",
    "</div> <br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th style=\"text-align: left\">Option</th>\n",
    "<th style=\"text-align: left\">Description /  CUBE.AI recommendation</th>\n",
    "\n",
    "</tr>\n",
    "<tr>\n",
    "    \n",
    "    \n",
    "<td style=\"text-align: left\">model</td>\n",
    "<td style=\"text-align: left\">model name corresponding to the file name uploaded</td>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td style=\"text-align: left\">optimization</td>\n",
    "<td style=\"text-align: left\">optimization setting \"balanced\", \"time\" or \"ram\"</td>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td style=\"text-align: left\">allocateInputs</td>\n",
    "<td style=\"text-align: left\"><strong>recommended</strong>, activations buffer will be also used to handle the input buffers.True by default</td>\n",
    "</tr>\n",
    " \n",
    "<tr>\n",
    "<td style=\"text-align: left\">allocateOutputs</td>\n",
    "<td style=\"text-align: left\"><strong>recommended</strong>, activations buffer will be also used to handle the output buffers. True by default</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align: left\">relocatable</td>\n",
    "<td style=\"text-align: left\"><strong>recommended</strong>, to generate a relocatable binary model. '--binary' option can be used to have a separate binary file with only the data of the weight/bias tensors. True by default</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align: left\">noOnnxOptimizer</td>\n",
    "<td style=\"text-align: left\"><strong>not recommended</strong>, allows to disable the ONNX optimizer pass. \"False\" by default. Apply only to ONNX file will be ignored otherwise</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align: left\">noOnnxIoTranspose</td>\n",
    "<td style=\"text-align: left\"> <strong>recommended only if</strong> the onnx model has already IO transpose layers to make it expect channel last data, allows to avoid adding a specific transpose layer during the import of a ONNX model, \"False\" by default. Apply only to ONNX file will be ignored otherwise</td>\n",
    "</tr>\n",
    "    \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_footprints(report=None):\n",
    "    activations_ram = report.ram_size / 1024\n",
    "    weights_rom = report.rom_size / 1024\n",
    "    macc = report.macc / 1e6\n",
    "    print(\"[INFO] : STM32Cube.AI model memory footprint\")\n",
    "    print(\"[INFO] : MACCs : {} (M)\".format(macc))\n",
    "    print(\"[INFO] : Flash Weights  : {0:.1f} (KiB)\".format(weights_rom))\n",
    "    print(\"[INFO] : RAM Activations : {0:.1f} (KiB)\".format(activations_ram))\n",
    "\n",
    "# Analyze RAM/Flash model memory footprints after optimization by STM32Cube.AI\n",
    "res_analyse = stmai.analyze(CliParameters(model=model_name, \\\n",
    "                                          optimization='balanced', \\\n",
    "                                          noOnnxIoTranspose=False, \\\n",
    "                                          relocatable=True, \\\n",
    "                                          noOnnxOptimizer=True))\n",
    "\n",
    "analyze_footprints(report=res_analyse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Benchmark\">\n",
    "        <h3> 4.3 Benchmark the model on a STM32 </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_inference_time(report=None):\n",
    "    cycles = report.cycles\n",
    "    inference_time = report.duration_ms\n",
    "    fps = 1000.0/inference_time\n",
    "    print(\"[INFO] : Number of cycles : {} \".format(cycles))\n",
    "    print(\"[INFO] : Inference Time : {0:.1f} (ms)\".format(inference_time))\n",
    "    print(\"[INFO] : FPS : {0:.1f}\".format(fps))\n",
    "    return fps\n",
    "\n",
    "try:\n",
    "  stmai.upload_model(model_path)\n",
    "  print(f'Model {model_name} is uploaded !')\n",
    "except Exception as e:\n",
    "    print(\"ERROR: \", e)\n",
    "fps_array=[]\n",
    "board_name='STM32H747I-DISCO'\n",
    "result = stmai.benchmark(CliParameters(model=model_name, \\\n",
    "                                           optimization='balanced', \\\n",
    "                                           allocateInputs=True, \\\n",
    "                                           allocateOutputs=True, \\\n",
    "                                           noOnnxIoTranspose=False, \\\n",
    "                                           fromModel=model_name), \\\n",
    "                                           board_name=board_name)\n",
    "fps = analyze_inference_time(report=result)\n",
    "fps_array.append(fps)\n",
    "# Save the result in outputs folder\n",
    "with open(f'./outputs/{model_name}_{board_name}.txt', 'w') as file_benchmark:\n",
    "      file_benchmark.write(f'{result}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"generate\">\n",
    "        <h3> 4.4 Generate the model optimized C code for STM32 </h3>\n",
    "</div>\n",
    "\n",
    "Here you generate the specialized network and data C-files to make the model ready to be integrated in the **STM32** application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_name='STM32H7'\n",
    "IDE='gcc'\n",
    "print(f'{model_name}\\ngenerating code for {board_name}')\n",
    "os.makedirs(f'code_outputs', exist_ok=True)\n",
    "# Generate model .c/.h code + Lib/Inc on STM32Cube.AI Developer Cloud\n",
    "result = stmai.generate(CliParameters(model=model_name, \\\n",
    "                                      output=\"code_outputs\", \\\n",
    "                                      optimization='balanced', \\\n",
    "                                      allocateInputs=True, \\\n",
    "                                      allocateOutputs=True, \\\n",
    "                                      noOnnxIoTranspose=False, \\\n",
    "                                      includeLibraryForSerie=CliLibrarySerie(board_name), \\\n",
    "                                      includeLibraryForIde=CliLibraryIde(IDE), \\\n",
    "                                      fromModel=model_name), \\\n",
    "                                      )\n",
    "\n",
    "print(os.listdir(\"./code_outputs\"))\n",
    "# print 20 first lines of the report\n",
    "with open('./code_outputs/network_generate_report.txt', 'r') as f: \n",
    "  for _ in range(20): print(next(f))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
